The Technical Interview: Algorithms and Data Structures
Algorithm - a program that solves a problem.

Introduction and Efficiency

Efficiency: also called complexity - how well computer resources of time and space are used
Time: how long does the code take to run
Space: how much storage space does your code occupy

Notation of Efficiency - for both time and space efficiency:

Big O Notation: O(1), O(log(n), O(sqrt(n)) O(n), O(n * log(n)), O(n^2), O(n^3), etc.
where n is the length of the input to the function. (Size of list or data)

List-Based Collections:

Lists/Arrays:
Arrays: arrays have indices, numbered from 0 to n - 1
Insertion is O(n)

Linked Lists:
No indices. Each element knows something about the next element (that it is linked to)
In each element, you store a reference (the memory address) to the next element.
Insertion is O(1)

Stacks:
First in, last out. (aka Last in, first out.)
Push to add element to top of stack, pop to remove element from top of stack.
Both push and pop run in constant time, O(1)

Queues:
First in, first out.
Add element to tail - Enqueue
Remove element from head - Dequeue
Look at head element but don't remove it - Peek
Like a linked list. Save references to head and tail so you can look them both up in O(1), constant time.
Priority Queue: assign each element a priority when you insert it into the Queue. This breaks the rules a bit because when you remove, you remove by priority first. If you have two or more with same priority, however, you remove the oldest element first.


Searching and Sorting:
Binary Search:
Look at middle element first, if value you seek is larger, look to right, if smaller, look to left. Again, look at middle of this new array to see if sought value is larger or smaller.
Efficiency of binary search: O(log(n)) (logarithmic)

Recursion:
Function calls itself. Base case tells the function when to stop recurring and return

Sorting:
Changing order of elements according to same rule, shortest to tallest, biggest to smallest, etc.
Can compare every element to every other element - naive approach. Works but slow and inelegant
For interviews: Have runtimes memorized to answer complexity problems really quickly. Also, consider whether the sorting algorithm is in-place or not. In-place sorting has low space complexity since there is no need to recreate the data structure, however the trade off is often for higher time complexity. With small arrays, this makes no difference. But with arrays of millions or billions of numbers, for instance...
Also, something to consider, in-place sorting is destructive... destroys the input array by replacing it with the sorted array.

Bubble Sort:
Naive approach: Smallest to largest.
Compare first two elements, if first is bigger, switch them. Now compare second and third element. Switch if needed. Once again, third and fourth elements. And so on.

In each iteration, the largest element in the array "bubbles up" to the top (to the last element in the array)

Naive Bubble Sort = O(n^2), polynomial time, in-place sorting, space complexity: O(1)

Merge Sort:
Split a huge array down as much as possible, then overtime rebuild it, sorting all the parts with each rebuilding. Divide and conquer.
Time efficiency: O(n * log(n))
Space complexity / Auxilliary space: O(n)

Quick Sort:
One of the most efficient sorting algorithms, but bad with arrays that are already nearly sorted.
Pick one of the values in the array at random (convention is to pick last element in the array), move all values smaller below it, all values larger above it. Continue recursively, picking a pivot in upper and lower array, sorting them similarly until the whole array is sorted.
Time efficiency: worst case O(n^2), best case O(n * log(n))
Space complexity: O(1) - in-place sorting.

Splitting array can sort more quickly.


Maps and Hashing:
Sets: comparable to a list, but big difference: Lists have some kind of ordering. Sets don't have ordering, but they contain only unique elements. A map is a set-based data structure, an array is a list-based data structure. The keys in a map are a set.

Maps: Maps are also called dictionaries.
Maps contain unique keys, and the keys in a map with no order are called a set.

Hashing:
Data structures that employ hash functions allow you to do lookups in O(1) constsant time.
Hash function transforms a value into a hash value based on some formula and spits out a coded version that's often the index of an array.

Collisions:
When a hash function spits out the same hash value for two different inputs.
Fixes:
1. Change the value in your hash function, or change the function completely - this requires moving to new arrays everytime you have a collision, which increases complexity of both time and space, but especially space. This is still O(1)
2. Change the structure of your array, like storing a collection in a bucket at each slot rather than the value itself. Here we'll iterate through small containers after looking up the container, which adds a bit of time. Best case, O(m) where m is the size of the bucket you look up in constant time. Worst case, this can end up as O(n) if all of your values end up in separate buckets.
Often when building a hash, you must choose between these two... take up much more space, search faster or take up less space, but take more time to search.
Could also use a hash within a hash to hash the buckets.

Hashing Conventions

Trees

Trees

Tree Traversal

Binary Trees

Binary Search Trees

Heaps

Self-Balancing Trees

Graphs

Graphs

Graph Properties

Graph Representation

Graph Traversal

Graph Paths

Case Studies in Algorithms

Shortest Path Problem

Knapsack Problem

Traveling Salesman Problem

Technical Interview Tips

Mock Interview Breakdown

Additional Tips

Practice with Pramp

Next Steps