The Technical Interview: Algorithms and Data Structures
Algorithm - a program that solves a problem.

Introduction and Efficiency

Efficiency: also called complexity - how well computer resources of time and space are used
Time: how long does the code take to run
Space: how much storage space does your code occupy

Notation of Efficiency - for both time and space efficiency:

Big O Notation: O(1), O(log(n), O(sqrt(n)) O(n), O(n * log(n)), O(n^2), O(n^3), etc.
where n is the length of the input to the function. (Size of list or data)

List-Based Collections:

Lists/Arrays:
Arrays: arrays have indices, numbered from 0 to n - 1
Insertion is O(n)

Linked Lists:
No indices. Each element knows something about the next element (that it is linked to)
In each element, you store a reference (the memory address) to the next element.
Insertion is O(1)

Stacks:
First in, last out. (aka Last in, first out.)
Push to add element to top of stack, pop to remove element from top of stack.
Both push and pop run in constant time, O(1)

Queues:
First in, first out.
Add element to tail - Enqueue
Remove element from head - Dequeue
Look at head element but don't remove it - Peek
Like a linked list. Save references to head and tail so you can look them both up in O(1), constant time.
Priority Queue: assign each element a priority when you insert it into the Queue. This breaks the rules a bit because when you remove, you remove by priority first. If you have two or more with same priority, however, you remove the oldest element first.


Searching and Sorting:
Binary Search:
Look at middle element first, if value you seek is larger, look to right, if smaller, look to left. Again, look at middle of this new array to see if sought value is larger or smaller.
Efficiency of binary search: O(log(n)) (logarithmic)

Recursion:
Function calls itself. Base case tells the function when to stop recurring and return

Sorting:
Changing order of elements according to same rule, shortest to tallest, biggest to smallest, etc.
Can compare every element to every other element - naive approach. Works but slow and inelegant
For interviews: Have runtimes memorized to answer complexity problems really quickly. Also, consider whether the sorting algorithm is in-place or not. In-place sorting has low space complexity since there is no need to recreate the data structure, however the trade off is often for higher time complexity. With small arrays, this makes no difference. But with arrays of millions or billions of numbers, for instance...
Also, something to consider, in-place sorting is destructive... destroys the input array by replacing it with the sorted array.

Bubble Sort:
Naive approach: Smallest to largest.
Compare first two elements, if first is bigger, switch them. Now compare second and third element. Switch if needed. Once again, third and fourth elements. And so on.

In each iteration, the largest element in the array "bubbles up" to the top (to the last element in the array)

Naive Bubble Sort = O(n^2), polynomial time, in-place sorting, space complexity: O(1)

Merge Sort:
Split a huge array down as much as possible, then overtime rebuild it, sorting all the parts with each rebuilding. Divide and conquer.
Time efficiency: O(n * log(n))
Space complexity / Auxilliary space: O(n)

Quick Sort:
One of the most efficient sorting algorithms, but bad with arrays that are already nearly sorted.
Pick one of the values in the array at random (convention is to pick last element in the array), move all values smaller below it, all values larger above it. Continue recursively, picking a pivot in upper and lower array, sorting them similarly until the whole array is sorted.
Time efficiency: worst case O(n^2), best case O(n * log(n))
Space complexity: O(1) - in-place sorting.

Splitting array can sort more quickly.


Maps and Hashing:
Sets: comparable to a list, but big difference: Lists have some kind of ordering. Sets don't have ordering, but they contain only unique elements. A map is a set-based data structure, an array is a list-based data structure. The keys in a map are a set.

Maps: Maps are also called dictionaries.
Maps contain unique keys, and the keys in a map with no order are called a set.

Hashing:
Data structures that employ hash functions allow you to do lookups in O(1) constsant time.
Hash function transforms a value into a hash value based on some formula and spits out a coded version that's often the index of an array.

Collisions:
When a hash function spits out the same hash value for two different inputs.
Fixes:
1. Change the value in your hash function, or change the function completely - this requires moving to new arrays everytime you have a collision, which increases complexity of both time and space, but especially space. This is still O(1)
2. Change the structure of your array, like storing a collection in a bucket at each slot rather than the value itself. Here we'll iterate through small containers after looking up the container, which adds a bit of time. Best case, O(m) where m is the size of the bucket you look up in constant time. Worst case, this can end up as O(n) if all of your values end up in separate buckets.
Often when building a hash, you must choose between these two... take up much more space, search faster or take up less space, but take more time to search.
Could also use a hash within a hash to hash the buckets.

Hashing Conventions
Load Factor = Number of Entries / Number of Buckets

Hashing? - Hash Table
Algorithm? - Hash Map - for constant time look ups to speed up the code.

String Keys:
Like ASCII, A = 65, B = 66, C = 67...
30 or fewer words, you can pobably get away with just ASCII value for first letter of a string as a hash value.

31 as a hash value is a convention, rather than the best value.

Hash Maps:

Trees:

Trees:
Trees start from a place called a Root. Data added called Branches. Bottom of Trees are called Leaves. Collection of Trees is called a Forest.

Trees are like LinkedLists, each node contains references to its children.

All elements in a tree are connected, but there are no cycles allowed.

Levels: Root at level 1, nodes connected to root are level 2, and so on down. Ancestor / parent, descendant / children.
Nodes at the end with no children are called Leaves or external nodes.
A parent node is called an internal node.
Connections are called Edges. Multiple edges are called a path. The height of a node is the number of edges between it and the furthest leave on that branch. A leaf has a height of zero, its parent has a height of one.
The height of a tree over all is the height of a root. The depth of a node is the number of edges to the root. Depth is the inverse of height.

Tree Traversal:
DFS - Depth first search: if there are children nodes to explore, exploring them is the priority.
Pre-Order Traversal - check off a node as soon as you see it, before you traverse any further in the tree. Start at root, check it off immediately, then check one of the children, normally the leftmost by convention. Check it off. Then continue traversing down the leftmost child nodes of each parent until you hit a leaf, then you go back up to the parent and over to the right leaf, and check it off, too. Then back up to the parent that has more children, down the left again if unchecked, then right, etc.

In-Order Traversal - Can only check off a node once we've seen its left child and come back to it, all the way down to left most leaf first. Then back up to the parent, check it off, then down to the right leaf, check it off, then back up to the parent's parent. We go more or less through all nodes on left first, then right, as we cross the tree.

Post-Order Traversal - Can only check off a node until we've seen all of its descendants, or we've visited both of its children and returned. So down to a leaf first, check it off, then to the right leaf, check it off, then back to parent to check it off. Root gets checked off very last.


BFS - Breadth first search: visit every node on the same level before visiting children nodes.
Level Order Traversal - start at the root, then visit its children on the second level, then visit their children on the 3rd level, until you've visited every single leaf. By convention, start on the left most node and move to the right.

Binary Trees:
Trees where parents have at most 2 children. Can have zero, one, or two children.

Search: O(n), must go through every single element if value you're seeking doesn't exist.
Delete: Often starts with a search. Deleting a leaf is quick and easy, but deleting a node with multiple children is trickier. But again, O(n). Linear runtime.
Insert: Easy, just tack it onto another node as long as we obey the no more than two children per nodes. O(logn). Logarithmic runtime. Worst case scenario is traversing down the longest path to last open node, down the levels.


Binary Search Trees (BST):
A specific type of binary tree where the nodes are sorted so that every value on the left of a particular node are smaller than it and every value on the right of a particular node are larger than it.

1-3-5-8-10
      |
      7

As a result of this, we can do operations like search, insert, and delete pretty quickly. Say we want to find 7... we start at the root, we see 7 is greater than 5, so we go right. Now we see 7 is less than 8, so we go left, and we find 7.

BST search is O(logn) (the height of the tree) (on average)

Say we want to insert 4. We start at 5, go left, see 4 is greater than 3, go right and insert.
1-3-5-8-10
  |   |
  4   7

BST insert is O(logn) (the height of the tree) (on average)

Deleting is complicated, but can be O(logn) (on average)

This is also a legitimate binary search tree:
5-10-15-20
There are two or fewer children at each parent and everything on the right is bigger than its parent.
It's called an unbalanced tree since the distribution of nodes is skewed, in this case to the right.
Unbalanced is the worst case scenario for a BST, because here Search, Insert, and Delete all take linear time.

Heaps
A specific type of tree:
Elements are arranged in increasing or decreasing order, such that the root element is either the maximum value (max heap) or minimum value (min heap) in the tree.

Max Heap: a parent must always have a greater value than its child
Min Heap: a parent must always have a lesser value than its child

Search, insert, and delete can vary a lot based on type of heap.

Heaps are not necessarily binary.

Binary heap must be a complete tree. All levels except the last level are completely full. If last level isn't totally full, values are added from left to right.

Function to get maximum value is called Peek. Runs in O(1) constant time for Binary heap.
Searching is linear O(n) worst case, Average case is about O(n/2) which is still approximately O(n). However, we can use the max heap properties to our advantage. For instance, if we are searching for a value larger than our max, then we can quit immediately without searching, we know it isn't in our tree.

Insertion: we stick the new element into the next open spot in the tree and then call Heapify.
Heapify is a function that reorders the tree based on the heap property. All we need to do to heapify up is compare the parent to its children and swap them until the heap properties are satisfied.

Extraction: Similar, stick the rightmost leaf into the spot where there is a value that must be removed from the heap. Then we compare it to its children and swap where necessary.

Runtime of insert and delete is O(logn) in the worst case. (moving an item up or downthe entire height of the tree)

Heap Implementation: Often represented as trees, but stored as arrays.
Arrays sorted in descending order can easily be turned into max heaps.
Arrays sorted in ascending order can easily be turned into min heaps.

Heaps are often stored in arrays because it saves space. When stored as a Tree, must store the value and pointers to left child, right child, and parent of each node. When stored as an array, only need to store value and the index in the array. Children and parents can be recovered from the index based on array index.

Self-Balancing Trees:
A balanced tree has nodes condensed to only a few levels. Unbalanced trees have nodes spread out amongst many levels. The most unbalanced tree is really just a LinkedList, where every node has only one child.

A self-balancing tree does some additional operations during insertion and deletion to keep itself balanced, the nodes themselves sometimes have some additional properties.

Red-Black Tree (A kind of self-balancing tree):
1. Nodes are assigned an additional color property where the nodes are either red or black. The use of color is just a convention, you really just need a way to distinguish between two different types of nodes.
2. Second property is the existence of null leaf nodes. Every node that does not have two leaves already must have null children.
3. Null leaf nodes are black. If a node is red, both children must be black.
4. Additional optional rule, root node must be black.
5. Every path from a node to its descendant null nodes must contain the same number of black nodes.

Red-Black Tree Insertion:
Upon insertion, tree must reorder itself to follow both Red-Black Tree and Binary Search Tree rules.
Insert a node only as a red node.
Null nodes are always added if there are no children or one null node and one child.
Case 1: insert node as red, then change to black if following the root must be black rule.
Case 2: when two children are inserted as red, they are switched to black and their parent becomes red. Root may change to red, but is then changed back to black.
Case 4: Perform left rotation shifting nodes one place to left if there are children on right side.
Case 5: Then do a right rotation to balance the tree.

Insert, search, and delete are O(logn) in the worst cases for Red-Black Trees.

Graphs / Networks:
A data structure designed to show relationships between objects.
(A tree is a type of graph)

Graph Properties:
Graphs have nodes or vertices.
Graphs have edges. Edges connect nodes. Nodes connected by edges are called adjacent.
Graphs can have cycles.

Both nodes and edges can contain data.
Edges often contain data about the strength of a connection.

Edges can have a direction, meaning the relationship only applies one way but not in the other. These are called Directed Graphs.

Connectivity:
Has a specific meaning in Graph Theory, but first a definition:
Disconnected graph means some portion of the graph is not connected to another.
A connected graph has no disconnected nodes.

Disconnected: A-B C-D-E
Connected: A-B-C-D-E

Connectivity: a way to describe a graph as a whole... a graph with a lot of connections between nodes is a graph with a lot of connectivity. Is more robust as well, can remove a node without destroying the graph or causing it to become disconnected.
Connectivity is the minimum number of elements that must be removed to cause a graph to become disconnected. Connectivity can sometimes be used to answer the question, "Which graph is stronger." Strongly connected graphs (cliques) must have a path from every node to every other node.

Directions:
Travel example:
SF -> Tokyo
From SF (noun, node) travel to (verb, edge, with direction) Tokyo (noun, node)

Cycles:
A cycle happens if you can travel from one node to others and then return to that node by a different path. Cycles are dangerous because they can lead to infinite loops.
On a Directed Graph, you might have two edges between each node, if different information is flowing in each direction.
An undirected graph has no directional information contained in its edges. It would just have one edge between nodes.
Often need to make sure you're taking in an acyclic graph as input.
DAG = Direct Acyclic Graph

Graph Representations:
Graphs can be functionally the same, but built in a number of different ways.
In OOP, graphs can have both vertex and edge objects. Each can have properties, for instance:
Vertex Object might have name, id, list of connected edges
Edge Object might have strength, id, list of connected vertices

Operations that traverse graphs can be more inconvenient if you must search through both nodes and edges.

One example of a simple graph that only uses lists:
Edge List: [[0,1], [1, 2], [1, 3], [2, 3]]   (The numbers correspond to ID numbers for the vertices)

0 - 1 - 2
    | /
    3

Sometimes called a 2D list. A list of lists of lists would be a 3D list.

Adjacency List for the graph above (again a 2D list):
[[1], [0,2,3], [1,3], [1,2]]
Array index = node id, then the values in that list are the ids of the other nodes that the node at that index is connected to. Here, node 0 is connected only to node 1, whereas node 1 is connected to nodes 0, 2, and 3.

Adjacency MAtrix:

[[0,1,0,0]
 [1,0,1,1]
 [0,1,0,1]
 [0,1,1,0]]

A matrix, also called a rectangular array, is a 2d array where all of the internal lists are the same length. The adjacency Matrix is really similar to the adjacency list, but information is represented differently. The inner lists now have one slot for every node in the graph. The indices of this array correspond to the edge connections, 0 for not connected, 1 for connected. The place where the row and column are the same (node 0 row, node 0 column for instance) is always 0, unless there is an edge that starts at that node and returns immediately to it in a loop. In the graph above, we see the list for node 0 shows connection to node 1, but no connection to 2 or 3. Node 1, the second list in the matrix, has connections to nodes 0, 2, and 3, but not to itself.

Important note: a single edge will show up twice in the matrix. Once for each node sharing that connection.


Graph Traversal:
Similar to tree traversal. Two basic methods, depth first search and breadth first search.
DFS: follow one path as far as it wil go.
BFS: look at all the node adjacent to a node before moving on to the next.

A traversal visits every node on a graph
A search is a traversal where you stop at the element you were seeking.

Depth First Search:
There's no root in a graph, so we can start anywhere. First mark the node we've selected as seen. Common implentations use the Stack.
So we mark the node as seen, add it to the stack.
Next, pick an edge, follow it, mark node as seen, and add it to the stack.

We explicitly visit each edge and vertex once in this algorithm, so runtime is O(|E| + |V|).

BFS: Done with Queue instead of Stack. Explore all adjacent nodes first, marking each node as seen and adding them to a queue. When we run out of edges, we deque that first node and check all adjacent nodes to it.

Runtime is O(|E| + |V|).


Graph Paths:
Eulerian path: Traversal that travels through each edge only once.
Eulerian cycle/tour: Traverse every path only once and end up at same node you started.
Runtime: O(|E) since it only visits each edge once.

Hamiltonian path: Visits every vertex once.
Hamiltonian cycle: Visits every vertex once, starts and ends on the same vertex.

Case Studies in Algorithms:

Shortest Path Problem:
Find the shortest path in a graph. Weighted edges. The shortest path between two nodes is the one where the sum of the edges is as small as possible. With an unweighted graph, the shortest path is just the path with the fewest edges.

Shortest path solution for unweighted graph is just a breadth first search.

Dijkstra's Algorithm: A solution to the shortest path problem for weighted undirected graphs.
We give all vertices a distance value, which is the sum of edge weights between our starting point and whatever vertex we're on. At the end of the algorithm, this distance will be the distance of the shortest path. We start with infinity for the distance value. Node we start with has a distance of 0. Min priority queue: element with min priority (min distance in this case), can be removed efficiently with the "Extract Min" function. So right away we extract the starting node, distance of zero. From starting node, we follow each edge and update the node on the other side with a distance value, which is just the weight of the edge. Which node to visit? Always visit node with the smallest value. Because we always pick the node with the lowest distance, Dijkstra's is called a Greedy algorithm. Philosphy is pick whatever looks best at the moment. We keep going, extracting the minimum from our queue, always following edges with smallest weights, updating our distance if we find a shorter path to a node.

Worst case runtime: O(|V|^2)

Knapsack Problem


Traveling Salesman Problem

Technical Interview Tips

Mock Interview Breakdown

Additional Tips

Practice with Pramp

Next Steps