The Technical Interview: Algorithms and Data Structures
Algorithm - a program that solves a problem.

Introduction and Efficiency

Efficiency: also called complexity - how well computer resources of time and space are used
Time: how long does the code take to run
Space: how much storage space does your code occupy

Notation of Efficiency - for both time and space efficiency:

Big O Notation: O(1), O(log(n), O(sqrt(n)) O(n), O(n * log(n)), O(n^2), O(n^3), etc.
where n is the length of the input to the function. (Size of list or data)

List-Based Collections:

Lists/Arrays:
Arrays: arrays have indices, numbered from 0 to n - 1
Insertion is O(n)

Linked Lists:
No indices. Each element knows something about the next element (that it is linked to)
In each element, you store a reference (the memory address) to the next element.
Insertion is O(1)

Stacks:
First in, last out. (aka Last in, first out.)
Push to add element to top of stack, pop to remove element from top of stack.
Both push and pop run in constant time, O(1)

Queues:
First in, first out.
Add element to tail - Enqueue
Remove element from head - Dequeue
Look at head element but don't remove it - Peek
Like a linked list. Save references to head and tail so you can look them both up in O(1), constant time.
Priority Queue: assign each element a priority when you insert it into the Queue. This breaks the rules a bit because when you remove, you remove by priority first. If you have two or more with same priority, however, you remove the oldest element first.


Searching and Sorting:
Binary Search:
Look at middle element first, if value you seek is larger, look to right, if smaller, look to left. Again, look at middle of this new array to see if sought value is larger or smaller.
Efficiency of binary search: O(log(n)) (logarithmic)

Recursion:
Function calls itself. Base case tells the function when to stop recurring and return

Sorting:
Changing order of elements according to same rule, shortest to tallest, biggest to smallest, etc.
Can compare every element to every other element - naive approach. Works but slow and inelegant
For interviews: Have runtimes memorized to answer complexity problems really quickly. Also, consider whether the sorting algorithm is in-place or not. In-place sorting has low space complexity since there is no need to recreate the data structure, however the trade off is often for higher time complexity. With small arrays, this makes no difference. But with arrays of millions or billions of numbers, for instance...
Also, something to consider, in-place sorting is destructive... destroys the input array by replacing it with the sorted array.

Bubble Sort:
Naive approach: Smallest to largest.
Compare first two elements, if first is bigger, switch them. Now compare second and third element. Switch if needed. Once again, third and fourth elements. And so on.

In each iteration, the largest element in the array "bubbles up" to the top (to the last element in the array)

Naive Bubble Sort = O(n^2), polynomial time, in-place sorting, space complexity: O(1)

Merge Sort:
Split a huge array down as much as possible, then overtime rebuild it, sorting all the parts with each rebuilding. Divide and conquer.
Time efficiency: O(n * log(n))
Space complexity / Auxilliary space: O(n)

Quick Sort:
One of the most efficient sorting algorithms, but bad with arrays that are already nearly sorted.
Pick one of the values in the array at random (convention is to pick last element in the array), move all values smaller below it, all values larger above it. Continue recursively, picking a pivot in upper and lower array, sorting them similarly until the whole array is sorted.
Time efficiency: worst case O(n^2), best case O(n * log(n))
Space complexity: O(1) - in-place sorting.

Splitting array can sort more quickly.


Maps and Hashing:
Sets: comparable to a list, but big difference: Lists have some kind of ordering. Sets don't have ordering, but they contain only unique elements. A map is a set-based data structure, an array is a list-based data structure. The keys in a map are a set.

Maps: Maps are also called dictionaries.
Maps contain unique keys, and the keys in a map with no order are called a set.

Hashing:
Data structures that employ hash functions allow you to do lookups in O(1) constsant time.
Hash function transforms a value into a hash value based on some formula and spits out a coded version that's often the index of an array.

Collisions:
When a hash function spits out the same hash value for two different inputs.
Fixes:
1. Change the value in your hash function, or change the function completely - this requires moving to new arrays everytime you have a collision, which increases complexity of both time and space, but especially space. This is still O(1)
2. Change the structure of your array, like storing a collection in a bucket at each slot rather than the value itself. Here we'll iterate through small containers after looking up the container, which adds a bit of time. Best case, O(m) where m is the size of the bucket you look up in constant time. Worst case, this can end up as O(n) if all of your values end up in separate buckets.
Often when building a hash, you must choose between these two... take up much more space, search faster or take up less space, but take more time to search.
Could also use a hash within a hash to hash the buckets.

Hashing Conventions
Load Factor = Number of Entries / Number of Buckets

Hashing? - Hash Table
Algorithm? - Hash Map - for constant time look ups to speed up the code.

String Keys:
Like ASCII, A = 65, B = 66, C = 67...
30 or fewer words, you can pobably get away with just ASCII value for first letter of a string as a hash value.

31 as a hash value is a convention, rather than the best value.

Hash Maps:

Trees:

Trees:
Trees start from a place called a Root. Data added called Branches. Bottom of Trees are called Leaves. Collection of Trees is called a Forest.

Trees are like LinkedLists, each node contains references to its children.

All elements in a tree are connected, but there are no cycles allowed.

Levels: Root at level 1, nodes connected to root are level 2, and so on down. Ancestor / parent, descendant / children.
Nodes at the end with no children are called Leaves or external nodes.
A parent node is called an internal node.
Connections are called Edges. Multiple edges are called a path. The height of a node is the number of edges between it and the furthest leave on that branch. A leaf has a height of zero, its parent has a height of one.
The height of a tree over all is the height of a root. The depth of a node is the number of edges to the root. Depth is the inverse of height.

Tree Traversal:
DFS - Depth first search: if there are children nodes to explore, exploring them is the priority.
Pre-Order Traversal - check off a node as soon as you see it, before you traverse any further in the tree. Start at root, check it off immediately, then check one of the children, normally the leftmost by convention. Check it off. Then continue traversing down the leftmost child nodes of each parent until you hit a leaf, then you go back up to the parent and over to the right leaf, and check it off, too. Then back up to the parent that has more children, down the left again if unchecked, then right, etc.

In-Order Traversal - Can only check off a node once we've seen its left child and come back to it, all the way down to left most leaf first. Then back up to the parent, check it off, then down to the right leaf, check it off, then back up to the parent's parent. We go more or less through all nodes on left first, then right, as we cross the tree.

Post-Order Traversal - Can only check off a node until we've seen all of its descendants, or we've visited both of its children and returned. So down to a leaf first, check it off, then to the right leaf, check it off, then back to parent to check it off. Root gets checked off very last.


BFS - Breadth first search: visit every node on the same level before visiting children nodes.
Level Order Traversal - start at the root, then visit its children on the second level, then visit their children on the 3rd level, until you've visited every single leaf. By convention, start on the left most node and move to the right.

Binary Trees:
Trees where parents have at most 2 children. Can have zero, one, or two children.

Binary Search Trees:
Search: O(n), must go through every single element if value you're seeking doesn't exist.
Delete: Often starts with a search. Deleting a leaf is quick and easy, but deleting a node with multiple children is trickier. But again, O(n). Linear runtime.
Insert: Easy, just tack it onto another node as long as we obey the no more than two children per nodes. O(logn). Logarithmic runtime. Worst case scenario is traversing down the longest path to last open node, down the levels.

Heaps

Self-Balancing Trees

Graphs

Graphs

Graph Properties

Graph Representation

Graph Traversal

Graph Paths

Case Studies in Algorithms

Shortest Path Problem

Knapsack Problem

Traveling Salesman Problem

Technical Interview Tips

Mock Interview Breakdown

Additional Tips

Practice with Pramp

Next Steps